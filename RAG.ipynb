{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain import PromptTemplate\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "from langchain.vectorstores import Chroma\n",
    "import chromadb\n",
    "\n",
    "import sys\n",
    "import os\n",
    "\n",
    "\n",
    "class SuppressStdout:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        self._original_stderr = sys.stderr\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "        sys.stderr = self._original_stderr\n",
    "\n",
    "# Load the PDF and convert it to text\n",
    "loader = PyPDFLoader(\"/workspaces/codespaces-blank/pdfs2/Nuno_CV.pdf\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Split the documents\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "\n",
    "# Now you can proceed with your vectorstore creation\n",
    "with SuppressStdout():\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"thenlper/gte-small\")\n",
    "    client = chromadb.Client()\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=all_splits,\n",
    "        embedding=embeddings,\n",
    "        client=client\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Question: Tell me about the candidate's CV experience with the Python programming language.\n",
      "\n",
      "Answer:\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The candidate is proficient in using Python for various tasks such as stand-ups, retrospective meetings, developing Django Projects integrating Dash and Plotly dashboards, working with genomics data sources like Clinvar, Ensembl, dbSNP, maintaining ETL pipelines utilizing Pandas and PySpark to implement machine learning algorithms for forecasting Laboratory Growth.\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"Tell me about the candidate's CV experience with the Python programming language.\"\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Use the following pieces of context to answer the question at the end.\n",
    "If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "Use three sentences maximum and keep the answer as concise as possible and format it in a way that it can be easily read by the user.\n",
    "{context}\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "QA_CHAIN_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=template,\n",
    ")\n",
    "\n",
    "# Create a custom callback handler\n",
    "class MultiLineStreamingCallbackHandler(StreamingStdOutCallbackHandler):\n",
    "    def __init__(self):\n",
    "        self.current_line = \"\"\n",
    "\n",
    "    def on_llm_new_token(self, token: str, **kwargs) -> None:\n",
    "        self.current_line += token\n",
    "        print(token, end=\"\", flush=True)\n",
    "        \n",
    "        if token in ['.', '!', '?']:\n",
    "            print(\"\\n\")\n",
    "            self.current_line = \"\"\n",
    "\n",
    "# Initialize the Ollama model with the custom callback handler\n",
    "llm = Ollama(\n",
    "    model=\"phi3\",\n",
    "    callback_manager=CallbackManager([MultiLineStreamingCallbackHandler()]),\n",
    "    num_ctx=2048\n",
    ")\n",
    "\n",
    "# Create the RetrievalQA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm,\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT},\n",
    ")\n",
    "\n",
    "# Run the query and print the result\n",
    "print(\"\\nQuestion:\", query)\n",
    "print(\"\\nAnswer:\\n\")\n",
    "result = qa_chain({\"query\": query})\n",
    "print(\"\\n\")  # Add a newline at the end for better readability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
